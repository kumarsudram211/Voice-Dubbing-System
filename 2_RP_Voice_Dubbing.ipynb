{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9sTq9wc4qRjdliwDOCDfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["print(\"=\" * 70)\n","print(\"INSTALLING DEPENDENCIES\")\n","print(\"=\" * 70)\n","\n","get_ipython().system('pip install -q librosa>=0.10.0 soundfile scipy numpy')\n","get_ipython().system('pip install -q openai-whisper')\n","get_ipython().system('pip install -q transformers sentencepiece protobuf')\n","get_ipython().system('pip install -q coqui-tts 2>&1 | grep -v \"WARNING\" || pip install -q git+https://github.com/suno-ai/bark.git')\n","get_ipython().system('pip install -q matplotlib seaborn')\n","\n","print(\"\\n Dependencies installed!\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxPSkiLcAwNR","executionInfo":{"status":"ok","timestamp":1762923954490,"user_tz":-330,"elapsed":118768,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"b9bcabc5-388d-4961-b581-830f91a8830d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","INSTALLING DEPENDENCIES\n","======================================================================\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/3.7 MB 5.4 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 kB 8.2 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.0/42.0 kB 3.0 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.6/101.6 kB 10.1 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/15.3 MB 3.0 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.1/18.1 MB 6.0 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.4/31.4 MB 3.2 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 2.4 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 858.4/858.4 kB 1.3 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.1/345.1 kB 2.4 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 kB 392.3 kB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 648.4/648.4 kB 1.2 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.5/163.5 kB 1.4 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.1/71.1 kB 236.0 kB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 1.3 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.8/293.8 kB 1.8 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.0 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 1.6 MB/s eta 0:00:00\n","\n"," Dependencies installed!\n","\n"]}]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import librosa\n","import soundfile as sf\n","import torch\n","import os\n","from pathlib import Path\n","from typing import Dict, Tuple, Optional, List\n","import json\n","from datetime import datetime\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import Audio, display, HTML, clear_output\n","\n","# Set style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (14, 8)\n","\n","print(\"=\" * 70)\n","print(\"LIBRARIES LOADED\")\n","print(\"=\" * 70)\n","print(f\"PyTorch: {torch.__version__}\")\n","print(f\"CUDA Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","print(\"=\" * 70 + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UG24Vog2BBd6","executionInfo":{"status":"ok","timestamp":1762923964180,"user_tz":-330,"elapsed":9685,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"f6dbcf0b-445a-4325-ece4-1315878dc18c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","LIBRARIES LOADED\n","======================================================================\n","PyTorch: 2.8.0+cu126\n","CUDA Available: True\n","GPU: Tesla T4\n","======================================================================\n","\n"]}]},{"cell_type":"code","source":["# SETUP: Initialize Global Variables\n","\n","# Global storage for pipeline data\n","PIPELINE_DATA = {\n","    'input_audio': None,\n","    'detected_language': None,\n","    'target_language': None,\n","    'transcribed_text': None,\n","    'translated_text': None,\n","    'synthesized_audio': None,\n","    'preserved_audio': None,\n","    'original_features': None,\n","    'synthesized_features': None,\n","    'preserved_features': None,\n","    'metrics': {},\n","    'processing_times': {}\n","}\n","\n","# Supported languages\n","SUPPORTED_LANGUAGES = {\n","    'en': {'name': 'English', 'model': None},\n","    'es': {'name': 'Spanish', 'model': 'Helsinki-NLP/opus-mt-en-es'},\n","    'fr': {'name': 'French', 'model': 'Helsinki-NLP/opus-mt-en-fr'},\n","    'de': {'name': 'German', 'model': 'Helsinki-NLP/opus-mt-en-de'},\n","    'it': {'name': 'Italian', 'model': 'Helsinki-NLP/opus-mt-en-it'},\n","    'pt': {'name': 'Portuguese', 'model': 'Helsinki-NLP/opus-mt-en-pt'},\n","    'ru': {'name': 'Russian', 'model': 'Helsinki-NLP/opus-mt-en-ru'},\n","    'ja': {'name': 'Japanese', 'model': 'Helsinki-NLP/opus-mt-en-jap'},\n","    'zh': {'name': 'Chinese', 'model': 'Helsinki-NLP/opus-mt-en-zh'},\n","    'ar': {'name': 'Arabic', 'model': 'Helsinki-NLP/opus-mt-en-ar'},\n","    'hi': {'name': 'Hindi', 'model': 'Helsinki-NLP/opus-mt-en-hi'},\n","    'ko': {'name': 'Korean', 'model': 'Helsinki-NLP/opus-mt-en-ko'},\n","    'nl': {'name': 'Dutch', 'model': 'Helsinki-NLP/opus-mt-en-nl'},\n","}\n","\n","print(\" Setup complete! Ready to start pipeline.\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhjkp1AcBj0p","executionInfo":{"status":"ok","timestamp":1762923964354,"user_tz":-330,"elapsed":170,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"8c51f8dd-132f-4312-87e0-3c4a9c9ec8c1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" Setup complete! Ready to start pipeline.\n","\n"]}]},{"cell_type":"code","source":["# STEP 1: FILE UPLOAD\n","\n","def step1_upload_file():\n","    \"\"\"Step 1: Upload audio file\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 1: FILE UPLOAD\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        from google.colab import files\n","        IN_COLAB = True\n","    except:\n","        IN_COLAB = False\n","\n","    if IN_COLAB:\n","        print(\"\\n Please select your audio file...\")\n","        print(\"   Supported formats: WAV, MP3, M4A, FLAC\")\n","        print(\"   Recommended: Clear speech, 5-30 seconds\\n\")\n","\n","        uploaded = files.upload()\n","\n","        if not uploaded:\n","            print(\"No file uploaded!\")\n","            return None\n","\n","        audio_file = list(uploaded.keys())[0]\n","    else:\n","        audio_file = input(\"Enter path to audio file: \")\n","\n","    if not os.path.exists(audio_file):\n","        print(f\"File not found: {audio_file}\")\n","        return None\n","\n","    # Get file info\n","    file_size_mb = os.path.getsize(audio_file) / (1024 * 1024)\n","\n","    # Load and analyze\n","    y, sr = librosa.load(audio_file, sr=None)\n","    duration = len(y) / sr\n","\n","    # Store in pipeline\n","    PIPELINE_DATA['input_audio'] = audio_file\n","\n","    print(f\"\\n File uploaded successfully!\")\n","    print(f\"\\n File Information:\")\n","    print(f\"   • Filename: {audio_file}\")\n","    print(f\"   • File size: {file_size_mb:.2f} MB\")\n","    print(f\"   • Duration: {duration:.2f} seconds\")\n","    print(f\"   • Sample rate: {sr} Hz\")\n","    print(f\"   • Channels: {'Mono' if len(y.shape) == 1 else 'Stereo'}\")\n","\n","    # Play audio\n","    print(f\"\\n Listen to uploaded audio:\")\n","    display(Audio(audio_file))\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 1 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n Next: Run step2_detect_language()\")\n","\n","    return audio_file"],"metadata":{"id":"WS9_qlsmBzPQ","executionInfo":{"status":"ok","timestamp":1762923964361,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# STEP 2: LANGUAGE DETECTION\n","\n","def step2_detect_language():\n","    \"\"\"Step 2: Detect language and transcribe\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 2: LANGUAGE DETECTION & TRANSCRIPTION\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['input_audio'] is None:\n","        print(\"Error: No audio file uploaded!\")\n","        print(\"Please run step1_upload_file() first\")\n","        return None\n","\n","    audio_file = PIPELINE_DATA['input_audio']\n","\n","    # Load Whisper model\n","    print(\"\\n Loading Whisper model...\")\n","    import whisper\n","    import time\n","\n","    start_time = time.time()\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = whisper.load_model(\"base\", device=device)\n","    load_time = time.time() - start_time\n","\n","    print(f\" Whisper loaded in {load_time:.2f} seconds\")\n","\n","    # Transcribe\n","    print(\"\\n Transcribing audio...\")\n","    start_time = time.time()\n","    result = model.transcribe(audio_file, verbose=False)\n","    transcribe_time = time.time() - start_time\n","\n","    detected_lang = result['language']\n","    transcribed_text = result['text']\n","\n","    # Store in pipeline\n","    PIPELINE_DATA['detected_language'] = detected_lang\n","    PIPELINE_DATA['transcribed_text'] = transcribed_text\n","    PIPELINE_DATA['processing_times']['transcription'] = transcribe_time\n","\n","    print(f\"Transcription complete in {transcribe_time:.2f} seconds\")\n","\n","    print(f\"\\n Results:\")\n","    print(f\"   • Detected Language: {detected_lang.upper()}\")\n","    print(f\"   • Language Name: {SUPPORTED_LANGUAGES.get(detected_lang, {}).get('name', 'Unknown')}\")\n","    print(f\"\\n   • Transcribed Text:\")\n","    print(f\"     \\\"{transcribed_text}\\\"\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 2 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step3_select_target_language('es')\")\n","\n","    return detected_lang, transcribed_text"],"metadata":{"id":"VufBAjxgCGVU","executionInfo":{"status":"ok","timestamp":1762923964367,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# STEP 3: TARGET LANGUAGE SELECTION\n","\n","def step3_select_target_language(target_lang: str = None):\n","    \"\"\"Step 3: Select target language for dubbing\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 3: TARGET LANGUAGE SELECTION\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['transcribed_text'] is None:\n","        print(\"Error: No transcription available!\")\n","        print(\"Please run step2_detect_language() first\")\n","        return None\n","\n","    # Show available languages\n","    print(\"\\n Available Target Languages:\")\n","    print(\"-\" * 70)\n","    for code, info in SUPPORTED_LANGUAGES.items():\n","        if info['model']:\n","            status = \"✓\" if code != PIPELINE_DATA['detected_language'] else \"⊗\"\n","            print(f\"   {status} {code:5s} → {info['name']}\")\n","    print(\"-\" * 70)\n","\n","    if target_lang is None:\n","        print(\"\\n Error: Please specify target language!\")\n","        print(\"   Example: step3_select_target_language('es')\")\n","        return None\n","\n","    if target_lang not in SUPPORTED_LANGUAGES:\n","        print(f\"\\n Error: Language '{target_lang}' not supported!\")\n","        return None\n","\n","    # Store selection\n","    PIPELINE_DATA['target_language'] = target_lang\n","\n","    print(f\"\\n Target Language Selected:\")\n","    print(f\"   • Code: {target_lang}\")\n","    print(f\"   • Name: {SUPPORTED_LANGUAGES[target_lang]['name']}\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 3 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step4_translate_text()\")\n","\n","    return target_lang"],"metadata":{"id":"LXpxg5kPCcOv","executionInfo":{"status":"ok","timestamp":1762923964372,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# STEP 4: TEXT TRANSLATION\n","\n","\"\"\"def step4_translate_text():\n","    Step 4: Translate text to target language\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 4: TEXT TRANSLATION\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['target_language'] is None:\n","        print(\"Error: No target language selected!\")\n","        return None\n","\n","    source_text = PIPELINE_DATA['transcribed_text']\n","    target_lang = PIPELINE_DATA['target_language']\n","\n","    print(f\"\\n Translation Setup:\")\n","    print(f\"   • Target: {target_lang} ({SUPPORTED_LANGUAGES[target_lang]['name']})\")\n","    print(f\"   • Text length: {len(source_text)} characters\")\n","\n","    # Load translation model\n","    print(f\"\\n Loading translation model...\")\n","    from transformers import MarianMTModel, MarianTokenizer\n","    import time\n","\n","    model_name = SUPPORTED_LANGUAGES[target_lang]['model']\n","\n","    start_time = time.time()\n","    tokenizer = MarianTokenizer.from_pretrained(model_name)\n","    model = MarianMTModel.from_pretrained(model_name)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","\n","    load_time = time.time() - start_time\n","    print(f\" Model loaded in {load_time:.2f} seconds\")\n","\n","    # Translate\n","    print(f\"\\n Translating...\")\n","    start_time = time.time()\n","\n","    inputs = tokenizer(source_text, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        translated = model.generate(**inputs)\n","\n","    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n","    translate_time = time.time() - start_time\n","\n","    # Store in pipeline\n","    PIPELINE_DATA['translated_text'] = translated_text\n","    PIPELINE_DATA['processing_times']['translation'] = translate_time\n","\n","    print(f\"Translation complete in {translate_time:.2f} seconds\")\n","\n","    print(f\"\\n Translation Results:\")\n","    print(f\"\\n   Original: \\\"{source_text}\\\"\")\n","    print(f\"\\n   Translated: \\\"{translated_text}\\\"\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 4 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step5_synthesize_speech()\")\n","\n","    return translated_text\"\"\"\n","\n","from transformers import MarianMTModel, MarianTokenizer\n","import torch\n","import time\n","\n","def step4_translate_text():\n","    \"\"\"Step 4: Translate text to target language (robust to missing model).\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 4: TEXT TRANSLATION\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['target_language'] is None:\n","        print(\" Error: No target language selected!\")\n","        return None\n","\n","    source_text = PIPELINE_DATA['transcribed_text']\n","    target_lang = PIPELINE_DATA['target_language']\n","    detected_lang = PIPELINE_DATA.get('detected_language', None)\n","\n","    # If target equals detected (or model is None), skip translation\n","    model_name = SUPPORTED_LANGUAGES.get(target_lang, {}).get('model', None)\n","    if (detected_lang is not None and target_lang == detected_lang) or model_name is None:\n","        print(\" Translation skipped because target language equals detected language or no model configured.\")\n","        PIPELINE_DATA['translated_text'] = source_text\n","        PIPELINE_DATA['processing_times']['translation'] = 0.0\n","        print(f\"   • Translated text set equal to source (length {len(source_text)} chars).\")\n","        print(\"\\n\" + \"=\" * 70)\n","        print(\" STEP 4 COMPLETE (skipped translation)\")\n","        print(\"=\" * 70)\n","        print(\"\\n  Next: Run step5_synthesize_speech()\")\n","        return source_text\n","\n","    # Otherwise load model and translate\n","    print(f\"\\n Translation Setup:   Target: {target_lang} ({SUPPORTED_LANGUAGES[target_lang]['name']})\")\n","    print(f\"   Text length: {len(source_text)} characters\")\n","    print(f\"\\n Loading translation model {model_name} ...\")\n","\n","    start_time = time.time()\n","    try:\n","        tokenizer = MarianTokenizer.from_pretrained(model_name)\n","        model = MarianMTModel.from_pretrained(model_name)\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        model.to(device)\n","    except Exception as e:\n","        print(f\" Error loading translation model: {e}\")\n","        print(\"Possible causes: invalid model name, network issues, or private Hugging Face repo.\")\n","        print(\"If model is private, run `huggingface-cli login` or provide an access token.\")\n","        return None\n","\n","    load_time = time.time() - start_time\n","    print(f\" Model loaded in {load_time:.2f} seconds\")\n","\n","    # Translate\n","    print(f\"\\n Translating...\")\n","    start_time = time.time()\n","    try:\n","        inputs = tokenizer(source_text, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        with torch.no_grad():\n","            translated = model.generate(**inputs)\n","        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n","    except Exception as e:\n","        print(f\" Error during translation: {e}\")\n","        return None\n","\n","    translate_time = time.time() - start_time\n","    PIPELINE_DATA['translated_text'] = translated_text\n","    PIPELINE_DATA['processing_times']['translation'] = translate_time\n","\n","    print(f\" Translation complete in {translate_time:.2f} seconds\")\n","    print(f\"\\n Translation Results:\")\n","    print(f\"\\n   Original: \\\"{source_text}\\\"\")\n","    print(f\"\\n   Translated: \\\"{translated_text}\\\"\")\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 4 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step5_synthesize_speech()\")\n","\n","    return translated_text\n","\n","# bind into global namespace\n","globals()['step4_translate_text'] = step4_translate_text\n","print(\" Patched step4_translate_text() with robust version.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8bW6v0-UCpe0","executionInfo":{"status":"ok","timestamp":1762923992017,"user_tz":-330,"elapsed":27642,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"999f20be-0518-45b1-ca86-cffdea064a76"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[" Patched step4_translate_text() with robust version.\n"]}]},{"cell_type":"code","source":["# STEP 5: TEXT-TO-SPEECH SYNTHESIS\n","\n","def step5_synthesize_speech():\n","    \"\"\"Step 5: Generate speech from translated text\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 5: TEXT-TO-SPEECH SYNTHESIS\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['translated_text'] is None:\n","        print(\"Error: No translated text available!\")\n","        return None\n","\n","    translated_text = PIPELINE_DATA['translated_text']\n","    target_lang = PIPELINE_DATA['target_language']\n","    reference_audio = PIPELINE_DATA['input_audio']\n","\n","    print(f\"\\n TTS Setup:\")\n","    print(f\"   • Text: \\\"{translated_text[:50]}...\\\"\")\n","    print(f\"   • Language: {target_lang}\")\n","\n","    # Load TTS model\n","    print(f\"\\n Loading TTS model...\")\n","    import time\n","\n","    start_time = time.time()\n","\n","    try:\n","        from TTS.api import TTS\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=(device==\"cuda\"))\n","        tts_engine = \"coqui\"\n","        print(f\" Coqui XTTS loaded\")\n","    except:\n","        from bark import preload_models\n","        preload_models()\n","        tts_engine = \"bark\"\n","        print(f\" Bark TTS loaded\")\n","\n","    load_time = time.time() - start_time\n","    print(f\"   Load time: {load_time:.2f} seconds\")\n","\n","    # Synthesize\n","    print(f\"\\n Synthesizing speech...\")\n","    output_file = \"synthesized_audio.wav\"\n","\n","    start_time = time.time()\n","\n","    if tts_engine == \"coqui\":\n","        tts.tts_to_file(\n","            text=translated_text,\n","            speaker_wav=reference_audio,\n","            language=target_lang,\n","            file_path=output_file\n","        )\n","    else:\n","        from bark import generate_audio, SAMPLE_RATE\n","        audio_array = generate_audio(translated_text)\n","        sf.write(output_file, audio_array, SAMPLE_RATE)\n","\n","    synthesis_time = time.time() - start_time\n","\n","    # Store in pipeline\n","    PIPELINE_DATA['synthesized_audio'] = output_file\n","    PIPELINE_DATA['processing_times']['synthesis'] = synthesis_time\n","\n","    print(f\" Synthesis complete in {synthesis_time:.2f} seconds\")\n","\n","    # Play audio\n","    print(f\"\\ Listen to synthesized audio:\")\n","    display(Audio(output_file))\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 5 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\  Next: Run step6_acoustic_analysis()\")\n","\n","    return output_file"],"metadata":{"id":"wP_TnCS5C49u","executionInfo":{"status":"ok","timestamp":1762923992025,"user_tz":-330,"elapsed":4,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# HELPER FUNCTIONS : Extract Acoustic Features\n","\n","def extract_detailed_features(audio_path: str, label: str = \"\") -> Dict:\n","    \"\"\"Extract comprehensive acoustic features\"\"\"\n","    y, sr = librosa.load(audio_path, sr=22050)\n","\n","    # Pitch\n","    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=50, fmax=500, sr=sr)\n","\n","    # Spectral\n","    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n","    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n","    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n","\n","    # Energy\n","    rms = librosa.feature.rms(y=y)[0]\n","\n","    # Temporal\n","    zcr = librosa.feature.zero_crossing_rate(y)[0]\n","    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n","\n","    # MFCC\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n","\n","    return {\n","        'audio': y,\n","        'sample_rate': sr,\n","        'duration': len(y) / sr,\n","        'f0': f0,\n","        'f0_mean': float(np.nanmean(f0)),\n","        'f0_std': float(np.nanstd(f0)),\n","        'f0_min': float(np.nanmin(f0)),\n","        'f0_max': float(np.nanmax(f0)),\n","        'spectral_centroid': spectral_centroid,\n","        'spectral_centroid_mean': float(np.mean(spectral_centroid)),\n","        'spectral_centroid_std': float(np.std(spectral_centroid)),\n","        'spectral_rolloff': spectral_rolloff,\n","        'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),\n","        'spectral_bandwidth': spectral_bandwidth,\n","        'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),\n","        'energy': rms,\n","        'energy_mean': float(np.mean(rms)),\n","        'energy_std': float(np.std(rms)),\n","        'dynamic_range': float(np.max(rms) / (np.min(rms) + 1e-8)),\n","        'zcr': zcr,\n","        'zcr_mean': float(np.mean(zcr)),\n","        'tempo': float(tempo),\n","        'mfcc': mfcc,\n","        'mfcc_mean': mfcc.mean(axis=1)\n","    }\n","\n","def compare_acoustic_features(features1: Dict, features2: Dict) -> Dict:\n","    \"\"\"Compare acoustic features and calculate similarity scores\"\"\"\n","    comparison = {}\n","\n","    # Pitch\n","    if not np.isnan(features1['f0_mean']) and not np.isnan(features2['f0_mean']):\n","        diff = abs(features1['f0_mean'] - features2['f0_mean'])\n","        match = max(0, 100 * (1 - diff / features1['f0_mean']))\n","        comparison['Pitch (F0) Mean'] = {\n","            'original': features1['f0_mean'],\n","            'synthesized': features2['f0_mean'],\n","            'match': match\n","        }\n","\n","    # Duration\n","    diff = abs(features1['duration'] - features2['duration'])\n","    match = max(0, 100 * (1 - diff / features1['duration']))\n","    comparison['Duration'] = {\n","        'original': features1['duration'],\n","        'synthesized': features2['duration'],\n","        'match': match\n","    }\n","\n","    # Spectral Centroid\n","    diff = abs(features1['spectral_centroid_mean'] - features2['spectral_centroid_mean'])\n","    match = max(0, 100 * (1 - diff / features1['spectral_centroid_mean']))\n","    comparison['Spectral Centroid'] = {\n","        'original': features1['spectral_centroid_mean'],\n","        'synthesized': features2['spectral_centroid_mean'],\n","        'match': match\n","    }\n","\n","    # Energy\n","    diff = abs(features1['energy_mean'] - features2['energy_mean'])\n","    match = max(0, 100 * (1 - diff / features1['energy_mean']))\n","    comparison['Energy Level'] = {\n","        'original': features1['energy_mean'],\n","        'synthesized': features2['energy_mean'],\n","        'match': match\n","    }\n","\n","    return comparison\n"],"metadata":{"id":"hvANwW69DMm1","executionInfo":{"status":"ok","timestamp":1762923992031,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# STEP 6: ACOUSTIC ANALYSIS\n","\n","def step6_acoustic_analysis():\n","    \"\"\"Step 6: Analyze acoustic features\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 6: ACOUSTIC ANALYSIS\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['synthesized_audio'] is None:\n","        print(\" Error: No synthesized audio available!\")\n","        return None\n","\n","    original_audio = PIPELINE_DATA['input_audio']\n","    synthesized_audio = PIPELINE_DATA['synthesized_audio']\n","\n","    print(f\"\\n Analyzing acoustic features...\")\n","\n","    print(f\"\\n Extracting features from original audio...\")\n","    original_features = extract_detailed_features(original_audio, \"Original\")\n","\n","    print(f\" Extracting features from synthesized audio...\")\n","    synthesized_features = extract_detailed_features(synthesized_audio, \"Synthesized\")\n","\n","    # Store in pipeline\n","    PIPELINE_DATA['original_features'] = original_features\n","    PIPELINE_DATA['synthesized_features'] = synthesized_features\n","\n","    # Compare\n","    comparison = compare_acoustic_features(original_features, synthesized_features)\n","\n","    print(f\"\\n Feature Comparison:\")\n","    print(\"-\" * 70)\n","    print(f\"{'Feature':<30} {'Original':>12} {'Synthesized':>12} {'Match':>8}\")\n","    print(\"-\" * 70)\n","\n","    for feature, values in comparison.items():\n","        print(f\"{feature:<30} {values['original']:>12.2f} {values['synthesized']:>12.2f} {values['match']:>7.1f}%\")\n","\n","    print(\"-\" * 70)\n","\n","    overall_score = np.mean([v['match'] for v in comparison.values()])\n","    print(f\"\\n Overall Acoustic Similarity: {overall_score:.1f}%\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 6 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\  Next: Run step7_acoustic_preservation()\")\n","\n","    return comparison"],"metadata":{"id":"uxCAUsfJDXj-","executionInfo":{"status":"ok","timestamp":1762923992039,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# STEP 7: ACOUSTIC PRESERVATION\n","\n","\n","def step7_acoustic_preservation():\n","    \"\"\"Step 7: Apply acoustic preservation\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 7: ACOUSTIC PRESERVATION\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['original_features'] is None:\n","        print(\" Error: No acoustic analysis available!\")\n","        return None\n","\n","    synthesized_audio = PIPELINE_DATA['synthesized_audio']\n","    original_features = PIPELINE_DATA['original_features']\n","\n","    print(f\"\\ Applying acoustic preservation...\")\n","\n","    import time\n","    start_time = time.time()\n","\n","    # Load synthesized audio\n","    y, sr = librosa.load(synthesized_audio, sr=original_features['sample_rate'])\n","\n","    print(f\"\\n   [1/5] Pitch adjustment...\")\n","    # Pitch transfer\n","    f0_current, _, _ = librosa.pyin(y, fmin=50, fmax=500, sr=sr)\n","    f0_current_mean = np.nanmean(f0_current)\n","    f0_target_mean = original_features['f0_mean']\n","\n","    if not np.isnan(f0_current_mean) and not np.isnan(f0_target_mean) and f0_current_mean > 0:\n","        semitones = 12 * np.log2(f0_target_mean / f0_current_mean)\n","        semitones = np.clip(semitones, -12, 12)\n","        y = librosa.effects.pitch_shift(y, sr=sr, n_steps=semitones)\n","        print(f\"       ✓ Pitch shifted by {semitones:.2f} semitones\")\n","\n","    print(f\"\\n   [2/5] Speaking rate adjustment...\")\n","    # Time stretching\n","    target_duration = original_features['duration']\n","    current_duration = len(y) / sr\n","    if current_duration > 0:\n","        stretch_rate = current_duration / target_duration\n","        stretch_rate = np.clip(stretch_rate, 0.75, 1.35)\n","        y = librosa.effects.time_stretch(y, rate=stretch_rate)\n","        print(f\"       ✓ Time stretched by {stretch_rate:.2f}x\")\n","\n","    print(f\"\\n   [3/5] Energy normalization...\")\n","    # Energy matching\n","    current_energy = np.sqrt(np.mean(y**2))\n","    target_energy = original_features['energy_mean']\n","    if current_energy > 0:\n","        energy_ratio = target_energy / current_energy\n","        energy_ratio = np.clip(energy_ratio, 0.5, 2.0)\n","        y = y * energy_ratio\n","        print(f\"       ✓ Energy adjusted by {energy_ratio:.2f}x\")\n","\n","    print(f\"\\n   [4/5] Spectral shaping...\")\n","    # Spectral shaping\n","    target_centroid = original_features['spectral_centroid_mean']\n","    current_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n","\n","    if current_centroid > 0:\n","        S = librosa.stft(y)\n","        freqs = librosa.fft_frequencies(sr=sr)\n","        filter_curve = np.exp(-(freqs - target_centroid)**2 / (2 * 2000**2))\n","        filter_curve = 0.3 + 0.7 * filter_curve\n","        S_filtered = S * filter_curve[:, np.newaxis]\n","        y = librosa.istft(S_filtered)\n","        print(f\"       ✓ Spectral shaping applied\")\n","\n","    print(f\"\\n   [5/5] Final normalization...\")\n","    # Final normalization\n","    max_val = np.max(np.abs(y))\n","    if max_val > 0:\n","        y = y / max_val * 0.95\n","        print(f\"       ✓ Normalized to 95% peak\")\n","\n","    # Save\n","    output_file = \"preserved_audio.wav\"\n","    sf.write(output_file, y, sr)\n","\n","    preservation_time = time.time() - start_time\n","    PIPELINE_DATA['preserved_audio'] = output_file\n","    PIPELINE_DATA['processing_times']['preservation'] = preservation_time\n","\n","    print(f\"\\n Acoustic preservation complete in {preservation_time:.2f} seconds\")\n","\n","    # Analyze preserved audio\n","    print(f\"\\n Analyzing preserved audio...\")\n","    preserved_features = extract_detailed_features(output_file, \"Preserved\")\n","    PIPELINE_DATA['preserved_features'] = preserved_features\n","\n","    # Play audio\n","    print(f\"\\n Listen to preserved audio:\")\n","    display(Audio(output_file))\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 7 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step8_performance_metrics()\")\n","\n","    return output_file"],"metadata":{"id":"ZYrc87GZDjwH","executionInfo":{"status":"ok","timestamp":1762923992048,"user_tz":-330,"elapsed":7,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# STEP 8: PERFORMANCE METRICS\n","\n","def step8_performance_metrics():\n","    \"\"\"Step 8: Calculate comprehensive performance metrics\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 8: PERFORMANCE METRICS & COMPARISON\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['preserved_features'] is None:\n","        print(\" Error: No preserved audio available!\")\n","        return None\n","\n","    orig = PIPELINE_DATA['original_features']\n","    synth = PIPELINE_DATA['synthesized_features']\n","    pres = PIPELINE_DATA['preserved_features']\n","\n","    print(\"\\n Calculating performance metrics...\")\n","\n","    # Calculate detailed metrics\n","    metrics = {\n","        'pitch_preservation': {},\n","        'temporal_preservation': {},\n","        'spectral_preservation': {},\n","        'energy_preservation': {},\n","        'overall_scores': {}\n","    }\n","\n","    # Pitch Metrics\n","    print(\"\\n[1/4] Pitch Preservation Metrics...\")\n","    if not np.isnan(orig['f0_mean']):\n","        synth_pitch_error = abs(synth['f0_mean'] - orig['f0_mean']) / orig['f0_mean'] * 100\n","        pres_pitch_error = abs(pres['f0_mean'] - orig['f0_mean']) / orig['f0_mean'] * 100\n","\n","        metrics['pitch_preservation'] = {\n","            'original_f0': orig['f0_mean'],\n","            'synthesized_f0': synth['f0_mean'],\n","            'preserved_f0': pres['f0_mean'],\n","            'synth_error_%': synth_pitch_error,\n","            'preserved_error_%': pres_pitch_error,\n","            'improvement_%': synth_pitch_error - pres_pitch_error,\n","            'synth_similarity_%': max(0, 100 - synth_pitch_error),\n","            'preserved_similarity_%': max(0, 100 - pres_pitch_error)\n","        }\n","\n","    # Temporal Metrics\n","    print(\"[2/4] Temporal Preservation Metrics...\")\n","    synth_duration_error = abs(synth['duration'] - orig['duration']) / orig['duration'] * 100\n","    pres_duration_error = abs(pres['duration'] - orig['duration']) / orig['duration'] * 100\n","\n","    synth_tempo_error = abs(synth['tempo'] - orig['tempo']) / orig['tempo'] * 100\n","    pres_tempo_error = abs(pres['tempo'] - orig['tempo']) / orig['tempo'] * 100\n","\n","    metrics['temporal_preservation'] = {\n","        'original_duration': orig['duration'],\n","        'synthesized_duration': synth['duration'],\n","        'preserved_duration': pres['duration'],\n","        'synth_duration_error_%': synth_duration_error,\n","        'preserved_duration_error_%': pres_duration_error,\n","        'duration_improvement_%': synth_duration_error - pres_duration_error,\n","        'original_tempo': orig['tempo'],\n","        'synthesized_tempo': synth['tempo'],\n","        'preserved_tempo': pres['tempo'],\n","        'synth_tempo_error_%': synth_tempo_error,\n","        'preserved_tempo_error_%': pres_tempo_error,\n","        'tempo_improvement_%': synth_tempo_error - pres_tempo_error\n","    }\n","\n","    # Spectral Metrics\n","    print(\"[3/4] Spectral Preservation Metrics...\")\n","    synth_centroid_error = abs(synth['spectral_centroid_mean'] - orig['spectral_centroid_mean']) / orig['spectral_centroid_mean'] * 100\n","    pres_centroid_error = abs(pres['spectral_centroid_mean'] - orig['spectral_centroid_mean']) / orig['spectral_centroid_mean'] * 100\n","\n","    synth_rolloff_error = abs(synth['spectral_rolloff_mean'] - orig['spectral_rolloff_mean']) / orig['spectral_rolloff_mean'] * 100\n","    pres_rolloff_error = abs(pres['spectral_rolloff_mean'] - orig['spectral_rolloff_mean']) / orig['spectral_rolloff_mean'] * 100\n","\n","    metrics['spectral_preservation'] = {\n","        'original_centroid': orig['spectral_centroid_mean'],\n","        'synthesized_centroid': synth['spectral_centroid_mean'],\n","        'preserved_centroid': pres['spectral_centroid_mean'],\n","        'synth_centroid_error_%': synth_centroid_error,\n","        'preserved_centroid_error_%': pres_centroid_error,\n","        'centroid_improvement_%': synth_centroid_error - pres_centroid_error,\n","        'original_rolloff': orig['spectral_rolloff_mean'],\n","        'synthesized_rolloff': synth['spectral_rolloff_mean'],\n","        'preserved_rolloff': pres['spectral_rolloff_mean'],\n","        'synth_rolloff_error_%': synth_rolloff_error,\n","        'preserved_rolloff_error_%': pres_rolloff_error,\n","        'rolloff_improvement_%': synth_rolloff_error - pres_rolloff_error\n","    }\n","\n","    # Energy Metrics\n","    print(\"[4/4] Energy Preservation Metrics...\")\n","    synth_energy_error = abs(synth['energy_mean'] - orig['energy_mean']) / orig['energy_mean'] * 100\n","    pres_energy_error = abs(pres['energy_mean'] - orig['energy_mean']) / orig['energy_mean'] * 100\n","\n","    synth_dr_error = abs(synth['dynamic_range'] - orig['dynamic_range']) / orig['dynamic_range'] * 100\n","    pres_dr_error = abs(pres['dynamic_range'] - orig['dynamic_range']) / orig['dynamic_range'] * 100\n","\n","    metrics['energy_preservation'] = {\n","        'original_energy': orig['energy_mean'],\n","        'synthesized_energy': synth['energy_mean'],\n","        'preserved_energy': pres['energy_mean'],\n","        'synth_energy_error_%': synth_energy_error,\n","        'preserved_energy_error_%': pres_energy_error,\n","        'energy_improvement_%': synth_energy_error - pres_energy_error,\n","        'original_dynamic_range': orig['dynamic_range'],\n","        'synthesized_dynamic_range': synth['dynamic_range'],\n","        'preserved_dynamic_range': pres['dynamic_range'],\n","        'synth_dr_error_%': synth_dr_error,\n","        'preserved_dr_error_%': pres_dr_error,\n","        'dr_improvement_%': synth_dr_error - pres_dr_error\n","    }\n","\n","    # Overall Scores\n","    synth_overall = 100 - np.mean([\n","        synth_pitch_error, synth_duration_error, synth_centroid_error, synth_energy_error\n","    ])\n","    pres_overall = 100 - np.mean([\n","        pres_pitch_error, pres_duration_error, pres_centroid_error, pres_energy_error\n","    ])\n","\n","    metrics['overall_scores'] = {\n","        'synthesized_similarity_%': max(0, synth_overall),\n","        'preserved_similarity_%': max(0, pres_overall),\n","        'overall_improvement_%': pres_overall - synth_overall\n","    }\n","\n","    PIPELINE_DATA['metrics'] = metrics\n","\n","    # Display Results\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"COMPREHENSIVE PERFORMANCE METRICS\")\n","    print(\"=\"*70)\n","\n","    print(\"\\n 1. PITCH PRESERVATION\")\n","    print(\"-\"*70)\n","    print(f\"Original F0:           {metrics['pitch_preservation']['original_f0']:.2f} Hz\")\n","    print(f\"Synthesized F0:        {metrics['pitch_preservation']['synthesized_f0']:.2f} Hz (Error: {metrics['pitch_preservation']['synth_error_%']:.1f}%)\")\n","    print(f\"Preserved F0:          {metrics['pitch_preservation']['preserved_f0']:.2f} Hz (Error: {metrics['pitch_preservation']['preserved_error_%']:.1f}%)\")\n","    print(f\"Improvement:           {metrics['pitch_preservation']['improvement_%']:.1f}%\")\n","\n","    print(\"\\n 2. TEMPORAL PRESERVATION\")\n","    print(\"-\"*70)\n","    print(f\"Original Duration:     {metrics['temporal_preservation']['original_duration']:.2f} sec\")\n","    print(f\"Synthesized Duration:  {metrics['temporal_preservation']['synthesized_duration']:.2f} sec (Error: {metrics['temporal_preservation']['synth_duration_error_%']:.1f}%)\")\n","    print(f\"Preserved Duration:    {metrics['temporal_preservation']['preserved_duration']:.2f} sec (Error: {metrics['temporal_preservation']['preserved_duration_error_%']:.1f}%)\")\n","    print(f\"Duration Improvement:  {metrics['temporal_preservation']['duration_improvement_%']:.1f}%\")\n","\n","    print(\"\\n 3. SPECTRAL PRESERVATION (Timbre)\")\n","    print(\"-\"*70)\n","    print(f\"Original Centroid:     {metrics['spectral_preservation']['original_centroid']:.0f} Hz\")\n","    print(f\"Synthesized Centroid:  {metrics['spectral_preservation']['synthesized_centroid']:.0f} Hz (Error: {metrics['spectral_preservation']['synth_centroid_error_%']:.1f}%)\")\n","    print(f\"Preserved Centroid:    {metrics['spectral_preservation']['preserved_centroid']:.0f} Hz (Error: {metrics['spectral_preservation']['preserved_centroid_error_%']:.1f}%)\")\n","    print(f\"Centroid Improvement:  {metrics['spectral_preservation']['centroid_improvement_%']:.1f}%\")\n","\n","    print(\"\\n 4. ENERGY PRESERVATION\")\n","    print(\"-\"*70)\n","    print(f\"Original Energy:       {metrics['energy_preservation']['original_energy']:.4f}\")\n","    print(f\"Synthesized Energy:    {metrics['energy_preservation']['synthesized_energy']:.4f} (Error: {metrics['energy_preservation']['synth_energy_error_%']:.1f}%)\")\n","    print(f\"Preserved Energy:      {metrics['energy_preservation']['preserved_energy']:.4f} (Error: {metrics['energy_preservation']['preserved_energy_error_%']:.1f}%)\")\n","    print(f\"Energy Improvement:    {metrics['energy_preservation']['energy_improvement_%']:.1f}%\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\" OVERALL ACOUSTIC PRESERVATION SCORES\")\n","    print(\"=\"*70)\n","    print(f\"Synthesized Audio:     {metrics['overall_scores']['synthesized_similarity_%']:.1f}%\")\n","    print(f\"Preserved Audio:       {metrics['overall_scores']['preserved_similarity_%']:.1f}%\")\n","    print(f\"Overall Improvement:   {metrics['overall_scores']['overall_improvement_%']:.1f}%\")\n","    print(\"=\"*70)\n","\n","    # Quality Assessment\n","    pres_score = metrics['overall_scores']['preserved_similarity_%']\n","    if pres_score >= 85:\n","        quality = \" EXCELLENT - Professional quality acoustic preservation\"\n","    elif pres_score >= 75:\n","        quality = \"✓ GOOD - High quality acoustic preservation\"\n","    elif pres_score >= 65:\n","        quality = \" MODERATE - Acceptable acoustic preservation\"\n","    else:\n","        quality = \" LOW - Consider adjusting preservation parameters\"\n","\n","    print(f\"\\nQuality Assessment: {quality}\\n\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\" STEP 8 COMPLETE\")\n","    print(\"=\" * 70)\n","    print(\"\\n  Next: Run step9_visualization()\")\n","\n","    return metrics"],"metadata":{"id":"U_pOwZKrDzcu","executionInfo":{"status":"ok","timestamp":1762923992053,"user_tz":-330,"elapsed":10,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# STEP 9: VISUALIZATION & COMPARISON PLOTS\n","\n","def step9_visualization():\n","    \"\"\"Step 9: Generate comprehensive comparison visualizations\"\"\"\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STEP 9: VISUALIZATION & COMPARISON PLOTS\")\n","    print(\"=\" * 70)\n","\n","    if PIPELINE_DATA['metrics'] is None:\n","        print(\" Error: No metrics available!\")\n","        return None\n","\n","    orig = PIPELINE_DATA['original_features']\n","    synth = PIPELINE_DATA['synthesized_features']\n","    pres = PIPELINE_DATA['preserved_features']\n","\n","    print(\"\\n Generating comparison visualizations...\\n\")\n","\n","    # Create comprehensive figure\n","    fig = plt.figure(figsize=(20, 16))\n","    gs = fig.add_gridspec(5, 3, hspace=0.3, wspace=0.3)\n","\n","    # ===== ROW 1: WAVEFORMS =====\n","    print(\"[1/9] Waveform comparison...\")\n","    ax1 = fig.add_subplot(gs[0, 0])\n","    ax2 = fig.add_subplot(gs[0, 1])\n","    ax3 = fig.add_subplot(gs[0, 2])\n","\n","    time_orig = np.linspace(0, orig['duration'], len(orig['audio']))\n","    time_synth = np.linspace(0, synth['duration'], len(synth['audio']))\n","    time_pres = np.linspace(0, pres['duration'], len(pres['audio']))\n","\n","    ax1.plot(time_orig, orig['audio'], color='blue', alpha=0.7, linewidth=0.5)\n","    ax1.set_title('Original Waveform', fontsize=12, fontweight='bold')\n","    ax1.set_xlabel('Time (s)')\n","    ax1.set_ylabel('Amplitude')\n","    ax1.grid(True, alpha=0.3)\n","\n","    ax2.plot(time_synth, synth['audio'], color='orange', alpha=0.7, linewidth=0.5)\n","    ax2.set_title('Synthesized Waveform', fontsize=12, fontweight='bold')\n","    ax2.set_xlabel('Time (s)')\n","    ax2.set_ylabel('Amplitude')\n","    ax2.grid(True, alpha=0.3)\n","\n","    ax3.plot(time_pres, pres['audio'], color='green', alpha=0.7, linewidth=0.5)\n","    ax3.set_title('Preserved Waveform', fontsize=12, fontweight='bold')\n","    ax3.set_xlabel('Time (s)')\n","    ax3.set_ylabel('Amplitude')\n","    ax3.grid(True, alpha=0.3)\n","\n","    # ===== ROW 2: SPECTROGRAMS =====\n","    print(\"[2/9] Spectrogram comparison...\")\n","    ax4 = fig.add_subplot(gs[1, 0])\n","    ax5 = fig.add_subplot(gs[1, 1])\n","    ax6 = fig.add_subplot(gs[1, 2])\n","\n","    D_orig = librosa.amplitude_to_db(np.abs(librosa.stft(orig['audio'])), ref=np.max)\n","    D_synth = librosa.amplitude_to_db(np.abs(librosa.stft(synth['audio'])), ref=np.max)\n","    D_pres = librosa.amplitude_to_db(np.abs(librosa.stft(pres['audio'])), ref=np.max)\n","\n","    img1 = librosa.display.specshow(D_orig, y_axis='hz', x_axis='time', ax=ax4, cmap='viridis')\n","    ax4.set_title('Original Spectrogram', fontsize=12, fontweight='bold')\n","    plt.colorbar(img1, ax=ax4, format='%+2.0f dB')\n","\n","    img2 = librosa.display.specshow(D_synth, y_axis='hz', x_axis='time', ax=ax5, cmap='viridis')\n","    ax5.set_title('Synthesized Spectrogram', fontsize=12, fontweight='bold')\n","    plt.colorbar(img2, ax=ax5, format='%+2.0f dB')\n","\n","    img3 = librosa.display.specshow(D_pres, y_axis='hz', x_axis='time', ax=ax6, cmap='viridis')\n","    ax6.set_title('Preserved Spectrogram', fontsize=12, fontweight='bold')\n","    plt.colorbar(img3, ax=ax6, format='%+2.0f dB')\n","\n","    # ===== ROW 3: PITCH CONTOURS =====\n","    print(\"[3/9] Pitch contour comparison...\")\n","    ax7 = fig.add_subplot(gs[2, 0])\n","    ax8 = fig.add_subplot(gs[2, 1])\n","    ax9 = fig.add_subplot(gs[2, 2])\n","\n","    times_orig = np.linspace(0, orig['duration'], len(orig['f0']))\n","    times_synth = np.linspace(0, synth['duration'], len(synth['f0']))\n","    times_pres = np.linspace(0, pres['duration'], len(pres['f0']))\n","\n","    ax7.plot(times_orig, orig['f0'], color='blue', alpha=0.8, linewidth=2)\n","    ax7.axhline(orig['f0_mean'], color='red', linestyle='--', label=f\"Mean: {orig['f0_mean']:.1f} Hz\")\n","    ax7.set_title('Original Pitch (F0)', fontsize=12, fontweight='bold')\n","    ax7.set_xlabel('Time (s)')\n","    ax7.set_ylabel('Frequency (Hz)')\n","    ax7.set_ylim([50, 500])\n","    ax7.legend()\n","    ax7.grid(True, alpha=0.3)\n","\n","    ax8.plot(times_synth, synth['f0'], color='orange', alpha=0.8, linewidth=2)\n","    ax8.axhline(synth['f0_mean'], color='red', linestyle='--', label=f\"Mean: {synth['f0_mean']:.1f} Hz\")\n","    ax8.set_title('Synthesized Pitch (F0)', fontsize=12, fontweight='bold')\n","    ax8.set_xlabel('Time (s)')\n","    ax8.set_ylabel('Frequency (Hz)')\n","    ax8.set_ylim([50, 500])\n","    ax8.legend()\n","    ax8.grid(True, alpha=0.3)\n","\n","    ax9.plot(times_pres, pres['f0'], color='green', alpha=0.8, linewidth=2)\n","    ax9.axhline(pres['f0_mean'], color='red', linestyle='--', label=f\"Mean: {pres['f0_mean']:.1f} Hz\")\n","    ax9.set_title('Preserved Pitch (F0)', fontsize=12, fontweight='bold')\n","    ax9.set_xlabel('Time (s)')\n","    ax9.set_ylabel('Frequency (Hz)')\n","    ax9.set_ylim([50, 500])\n","    ax9.legend()\n","    ax9.grid(True, alpha=0.3)\n","\n","    # ===== ROW 4: SPECTRAL FEATURES =====\n","    print(\"[4/9] Spectral features comparison...\")\n","    ax10 = fig.add_subplot(gs[3, 0])\n","    ax11 = fig.add_subplot(gs[3, 1])\n","\n","    # Spectral Centroid\n","    times_orig_sc = np.linspace(0, orig['duration'], len(orig['spectral_centroid']))\n","    times_synth_sc = np.linspace(0, synth['duration'], len(synth['spectral_centroid']))\n","    times_pres_sc = np.linspace(0, pres['duration'], len(pres['spectral_centroid']))\n","\n","    ax10.plot(times_orig_sc, orig['spectral_centroid'], label='Original', color='blue', alpha=0.7, linewidth=2)\n","    ax10.plot(times_synth_sc, synth['spectral_centroid'], label='Synthesized', color='orange', alpha=0.7, linewidth=2)\n","    ax10.plot(times_pres_sc, pres['spectral_centroid'], label='Preserved', color='green', alpha=0.7, linewidth=2)\n","    ax10.set_title('Spectral Centroid (Timbre Brightness)', fontsize=12, fontweight='bold')\n","    ax10.set_xlabel('Time (s)')\n","    ax10.set_ylabel('Frequency (Hz)')\n","    ax10.legend()\n","    ax10.grid(True, alpha=0.3)\n","\n","    # Spectral Rolloff\n","    times_orig_sr = np.linspace(0, orig['duration'], len(orig['spectral_rolloff']))\n","    times_synth_sr = np.linspace(0, synth['duration'], len(synth['spectral_rolloff']))\n","    times_pres_sr = np.linspace(0, pres['duration'], len(pres['spectral_rolloff']))\n","\n","    ax11.plot(times_orig_sr, orig['spectral_rolloff'], label='Original', color='blue', alpha=0.7, linewidth=2)\n","    ax11.plot(times_synth_sr, synth['spectral_rolloff'], label='Synthesized', color='orange', alpha=0.7, linewidth=2)\n","    ax11.plot(times_pres_sr, pres['spectral_rolloff'], label='Preserved', color='green', alpha=0.7, linewidth=2)\n","    ax11.set_title('Spectral Rolloff', fontsize=12, fontweight='bold')\n","    ax11.set_xlabel('Time (s)')\n","    ax11.set_ylabel('Frequency (Hz)')\n","    ax11.legend()\n","    ax11.grid(True, alpha=0.3)\n","\n","    # Energy Envelope\n","    ax12 = fig.add_subplot(gs[3, 2])\n","    times_orig_e = np.linspace(0, orig['duration'], len(orig['energy']))\n","    times_synth_e = np.linspace(0, synth['duration'], len(synth['energy']))\n","    times_pres_e = np.linspace(0, pres['duration'], len(pres['energy']))\n","\n","    ax12.plot(times_orig_e, orig['energy'], label='Original', color='blue', alpha=0.7, linewidth=2)\n","    ax12.plot(times_synth_e, synth['energy'], label='Synthesized', color='orange', alpha=0.7, linewidth=2)\n","    ax12.plot(times_pres_e, pres['energy'], label='Preserved', color='green', alpha=0.7, linewidth=2)\n","    ax12.set_title('Energy Envelope', fontsize=12, fontweight='bold')\n","    ax12.set_xlabel('Time (s)')\n","    ax12.set_ylabel('RMS Energy')\n","    ax12.legend()\n","    ax12.grid(True, alpha=0.3)\n","\n","    # ===== ROW 5: METRICS COMPARISON =====\n","    print(\"[5/9] Metrics bar charts...\")\n","    ax13 = fig.add_subplot(gs[4, :])\n","\n","    metrics = PIPELINE_DATA['metrics']\n","\n","    categories = ['Pitch\\nPreservation', 'Duration\\nMatching', 'Spectral\\nSimilarity', 'Energy\\nMatching']\n","\n","    synth_scores = [\n","        metrics['pitch_preservation']['synth_similarity_%'],\n","        100 - metrics['temporal_preservation']['synth_duration_error_%'],\n","        100 - metrics['spectral_preservation']['synth_centroid_error_%'],\n","        100 - metrics['energy_preservation']['synth_energy_error_%']\n","    ]\n","\n","    pres_scores = [\n","        metrics['pitch_preservation']['preserved_similarity_%'],\n","        100 - metrics['temporal_preservation']['preserved_duration_error_%'],\n","        100 - metrics['spectral_preservation']['preserved_centroid_error_%'],\n","        100 - metrics['energy_preservation']['preserved_energy_error_%']\n","    ]\n","\n","    x = np.arange(len(categories))\n","    width = 0.35\n","\n","    bars1 = ax13.bar(x - width/2, synth_scores, width, label='Synthesized', color='orange', alpha=0.8)\n","    bars2 = ax13.bar(x + width/2, pres_scores, width, label='Preserved', color='green', alpha=0.8)\n","\n","    ax13.set_ylabel('Similarity Score (%)', fontsize=12, fontweight='bold')\n","    ax13.set_title('Acoustic Preservation Performance Comparison', fontsize=14, fontweight='bold')\n","    ax13.set_xticks(x)\n","    ax13.set_xticklabels(categories)\n","    ax13.legend(fontsize=11)\n","    ax13.grid(True, alpha=0.3, axis='y')\n","    ax13.set_ylim([0, 105])\n","\n","    # Add value labels on bars\n","    for bars in [bars1, bars2]:\n","        for bar in bars:\n","            height = bar.get_height()\n","            ax13.text(bar.get_x() + bar.get_width()/2., height,\n","                     f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n","\n","    # Overall title\n","    fig.suptitle('Comprehensive Acoustic Analysis: Original vs Synthesized vs Preserved',\n","                 fontsize=16, fontweight='bold', y=0.995)\n","\n","    plt.tight_layout()\n","\n","    print(\"[6/9] Saving visualization...\")\n","    plt.savefig('acoustic_comparison.png', dpi=300, bbox_inches='tight')\n","    print(\"       ✓ Saved as 'acoustic_comparison.png'\")\n","\n","    plt.show()\n","\n","    # ===== ADDITIONAL PLOT: Overall Similarity Radar Chart =====\n","    print(\"\\n[7/9] Generating radar chart...\")\n","\n","    fig2, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n","\n","    categories_radar = ['Pitch\\nF0', 'Duration', 'Spectral\\nCentroid', 'Energy\\nLevel',\n","                        'Dynamic\\nRange', 'Tempo']\n","\n","    synth_radar = [\n","        metrics['pitch_preservation']['synth_similarity_%'],\n","        100 - metrics['temporal_preservation']['synth_duration_error_%'],\n","        100 - metrics['spectral_preservation']['synth_centroid_error_%'],\n","        100 - metrics['energy_preservation']['synth_energy_error_%'],\n","        100 - metrics['energy_preservation']['synth_dr_error_%'],\n","        100 - metrics['temporal_preservation']['synth_tempo_error_%']\n","    ]\n","\n","    pres_radar = [\n","        metrics['pitch_preservation']['preserved_similarity_%'],\n","        100 - metrics['temporal_preservation']['preserved_duration_error_%'],\n","        100 - metrics['spectral_preservation']['preserved_centroid_error_%'],\n","        100 - metrics['energy_preservation']['preserved_energy_error_%'],\n","        100 - metrics['energy_preservation']['preserved_dr_error_%'],\n","        100 - metrics['temporal_preservation']['preserved_tempo_error_%']\n","    ]\n","\n","    angles = np.linspace(0, 2 * np.pi, len(categories_radar), endpoint=False).tolist()\n","    synth_radar += synth_radar[:1]\n","    pres_radar += pres_radar[:1]\n","    angles += angles[:1]\n","\n","    ax.plot(angles, synth_radar, 'o-', linewidth=2, label='Synthesized', color='orange')\n","    ax.fill(angles, synth_radar, alpha=0.25, color='orange')\n","\n","    ax.plot(angles, pres_radar, 'o-', linewidth=2, label='Preserved', color='green')\n","    ax.fill(angles, pres_radar, alpha=0.25, color='green')\n","\n","    ax.set_xticks(angles[:-1])\n","    ax.set_xticklabels(categories_radar, size=11)\n","    ax.set_ylim(0, 100)\n","    ax.set_yticks([20, 40, 60, 80, 100])\n","    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n","    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n","    ax.set_title('Acoustic Feature Preservation Radar Chart', size=14, fontweight='bold', pad=20)\n","    ax.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')\n","    print(\"       ✓ Saved as 'radar_comparison.png'\")\n","    plt.show()\n","\n","    # ===== ADDITIONAL PLOT: Processing Time Analysis =====\n","    print(\"\\n[8/9] Generating processing time chart...\")\n","\n","    times = PIPELINE_DATA['processing_times']\n","\n","    fig3, ax = plt.subplots(figsize=(10, 6))\n","\n","    stages = list(times.keys())\n","    durations = list(times.values())\n","    colors_stages = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n","\n","    bars = ax.barh(stages, durations, color=colors_stages, alpha=0.8)\n","\n","    # Add value labels\n","    for i, (bar, duration) in enumerate(zip(bars, durations)):\n","        ax.text(duration + 0.1, i, f'{duration:.2f}s', va='center', fontweight='bold')\n","\n","    ax.set_xlabel('Time (seconds)', fontsize=12, fontweight='bold')\n","    ax.set_title('Processing Time Breakdown', fontsize=14, fontweight='bold')\n","    ax.grid(True, alpha=0.3, axis='x')\n","\n","    total_time = sum(durations)\n","    ax.text(0.95, 0.95, f'Total: {total_time:.2f}s', transform=ax.transAxes,\n","            fontsize=12, fontweight='bold', ha='right', va='top',\n","            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","    plt.tight_layout()\n","    plt.savefig('processing_time.png', dpi=300, bbox_inches='tight')\n","    print(\"       ✓ Saved as 'processing_time.png'\")\n","    plt.show()\n","\n","    # ===== SUMMARY TABLE =====\n","    print(\"\\n[9/9] Generating summary table...\")\n","\n","    fig4, ax = plt.subplots(figsize=(12, 8))\n","    ax.axis('tight')\n","    ax.axis('off')\n","\n","    table_data = [\n","        ['Metric', 'Original', 'Synthesized', 'Preserved', 'Improvement'],\n","        ['─' * 20, '─' * 12, '─' * 12, '─' * 12, '─' * 12],\n","        ['Pitch (F0) Hz', f\"{orig['f0_mean']:.1f}\",\n","         f\"{synth['f0_mean']:.1f}\", f\"{pres['f0_mean']:.1f}\",\n","         f\"+{metrics['pitch_preservation']['improvement_%']:.1f}%\"],\n","        ['Duration (sec)', f\"{orig['duration']:.2f}\",\n","         f\"{synth['duration']:.2f}\", f\"{pres['duration']:.2f}\",\n","         f\"+{metrics['temporal_preservation']['duration_improvement_%']:.1f}%\"],\n","        ['Spectral Centroid (Hz)', f\"{orig['spectral_centroid_mean']:.0f}\",\n","         f\"{synth['spectral_centroid_mean']:.0f}\", f\"{pres['spectral_centroid_mean']:.0f}\",\n","         f\"+{metrics['spectral_preservation']['centroid_improvement_%']:.1f}%\"],\n","        ['Energy Level', f\"{orig['energy_mean']:.4f}\",\n","         f\"{synth['energy_mean']:.4f}\", f\"{pres['energy_mean']:.4f}\",\n","         f\"+{metrics['energy_preservation']['energy_improvement_%']:.1f}%\"],\n","        ['Dynamic Range', f\"{orig['dynamic_range']:.2f}\",\n","         f\"{synth['dynamic_range']:.2f}\", f\"{pres['dynamic_range']:.2f}\",\n","         f\"+{metrics['energy_preservation']['dr_improvement_%']:.1f}%\"],\n","        ['Tempo (BPM)', f\"{orig['tempo']:.1f}\",\n","         f\"{synth['tempo']:.1f}\", f\"{pres['tempo']:.1f}\",\n","         f\"+{metrics['temporal_preservation']['tempo_improvement_%']:.1f}%\"],\n","        ['─' * 20, '─' * 12, '─' * 12, '─' * 12, '─' * 12],\n","        ['Overall Similarity', '100%',\n","         f\"{metrics['overall_scores']['synthesized_similarity_%']:.1f}%\",\n","         f\"{metrics['overall_scores']['preserved_similarity_%']:.1f}%\",\n","         f\"+{metrics['overall_scores']['overall_improvement_%']:.1f}%\"]\n","    ]"],"metadata":{"id":"eu66lwECE2Sc","executionInfo":{"status":"ok","timestamp":1762923992090,"user_tz":-330,"elapsed":36,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#  PIPELINE HELPER + RUNNER CELL\n","\n","import os\n","from IPython.display import display, Audio, clear_output\n","try:\n","    from google.colab import files as colab_files\n","    IN_COLAB = True\n","except Exception:\n","    IN_COLAB = False\n","\n","def upload_file_interactive():\n","    \"\"\"Interactive upload for Colab or prompt path for local.\"\"\"\n","    if IN_COLAB:\n","        print(\" Colab upload: choose your audio file (wav/mp3/m4a/flac).\")\n","        uploaded = colab_files.upload()\n","        if not uploaded:\n","            print(\" No file uploaded.\")\n","            return None\n","        file_path = list(uploaded.keys())[0]\n","        print(f\" Uploaded: {file_path}\")\n","        return file_path\n","    else:\n","        p = input(\"Enter path to audio file: \").strip()\n","        return p if p else None\n","\n","def prepare_pipeline_with_file(path):\n","    \"\"\"Validate file exists, load basic info and set PIPELINE_DATA['input_audio']\"\"\"\n","    if not path:\n","        print(\" No path provided.\")\n","        return False\n","    if not os.path.exists(path):\n","        print(f\" File not found: {path}\")\n","        return False\n","    # store\n","    PIPELINE_DATA['input_audio'] = path\n","    # reset downstream fields\n","    for k in ['detected_language','target_language','transcribed_text','translated_text',\n","              'synthesized_audio','preserved_audio','original_features','synthesized_features',\n","              'preserved_features','metrics','processing_times']:\n","        if k in PIPELINE_DATA and k != 'processing_times':\n","            PIPELINE_DATA[k] = None\n","    PIPELINE_DATA['processing_times'] = {}\n","    print(f\" Pipeline prepared with file: {path}\")\n","    return True\n","\n","def download_if_colab(path):\n","    \"\"\"Trigger download in Colab if available.\"\"\"\n","    if IN_COLAB and os.path.exists(path):\n","        try:\n","            colab_files.download(path)\n","            print(f\" Downloaded: {path}\")\n","        except Exception as e:\n","            print(f\" Could not auto-download ({e}). File is at: {path}\")\n","\n","def run_full_pipeline(target_lang: str = 'es', force_tts: str = None, preserve_acoustics: bool = True):\n","    \"\"\"\n","    Run steps 1->9 in order.\n","    If PIPELINE_DATA['input_audio'] is None it will prompt upload (Colab) or path (local).\n","    Arguments:\n","      - target_lang: language code from SUPPORTED_LANGUAGES (default 'es')\n","      - force_tts: \"coqui\" or \"bark\" to force engine choice (optional)\n","      - preserve_acoustics: whether to apply preservation (step7)\n","    \"\"\"\n","    clear_output()\n","    print(\"=\"*70)\n","    print(\" RUNNING FULL VOICE DUBBING PIPELINE\")\n","    print(\"=\"*70)\n","    # Step 1: ensure input file present\n","    if not PIPELINE_DATA.get('input_audio'):\n","        print(\"No input file found in PIPELINE_DATA. Prompting upload...\")\n","        path = upload_file_interactive()\n","        ok = prepare_pipeline_with_file(path)\n","        if not ok:\n","            print(\"Aborting pipeline run.\")\n","            return\n","    else:\n","        print(f\"Input file already set: {PIPELINE_DATA['input_audio']}\")\n","    # Step 2: detect language / transcribe\n","    try:\n","        det, text = step2_detect_language()\n","    except Exception as e:\n","        print(f\" Error in step2_detect_language(): {e}\")\n","        return\n","    # Step 3: select target language\n","    if target_lang is None:\n","        print(\"No target language provided; defaulting to 'es' (Spanish).\")\n","        target_lang = 'es'\n","    if target_lang not in SUPPORTED_LANGUAGES:\n","        print(f\" Target language '{target_lang}' not supported. Aborting.\")\n","        return\n","    step3_select_target_language(target_lang)\n","    # Step 4: translation\n","    try:\n","        step4_translate_text()\n","    except Exception as e:\n","        print(f\" Error in step4_translate_text(): {e}\")\n","        return\n","    # Step 5: synthesis (allow forcing TTS)\n","    # If user wants to force engine, adjust code used in step5; easiest is to set a global flag and then call step5.\n","    if force_tts:\n","        print(f\" Forcing TTS engine to: {force_tts}\")\n","        # Try to set a tmp var used by step5 (it uses local variables).\n","        # Easiest approach: re-run step5 logic here to control engine explicitly.\n","        translated_text = PIPELINE_DATA['translated_text']\n","        target_lang_local = PIPELINE_DATA['target_language']\n","        reference_audio = PIPELINE_DATA['input_audio']\n","        print(\"\\n Loading user-selected TTS engine and synthesizing...\")\n","        try:\n","            # Force Coqui\n","            if force_tts.lower() == 'coqui':\n","                from TTS.api import TTS\n","                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","                tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=(device==\"cuda\"))\n","                tts.tts_to_file(text=translated_text, speaker_wav=reference_audio, language=target_lang_local, file_path=\"synthesized_audio.wav\")\n","            else:\n","                # bark\n","                from bark import generate_audio, SAMPLE_RATE\n","                audio_array = generate_audio(translated_text)\n","                sf.write(\"synthesized_audio.wav\", audio_array, SAMPLE_RATE)\n","            PIPELINE_DATA['synthesized_audio'] = \"synthesized_audio.wav\"\n","            print(\" Forced TTS synthesis complete.\")\n","            display(Audio(PIPELINE_DATA['synthesized_audio']))\n","        except Exception as e:\n","            print(f\" Forced TTS synthesis failed: {e}\")\n","            return\n","    else:\n","        # Call the defined step5\n","        try:\n","            step5_synthesize_speech()\n","        except Exception as e:\n","            print(f\" Error in step5_synthesize_speech(): {e}\")\n","            return\n","    # Step 6: Acoustic analysis (original vs synthesized)\n","    try:\n","        step6_acoustic_analysis()\n","    except Exception as e:\n","        print(f\" Error in step6_acoustic_analysis(): {e}\")\n","        return\n","    # Step 7: Acoustic preservation (optional)\n","    if preserve_acoustics:\n","        try:\n","            step7_acoustic_preservation()\n","        except Exception as e:\n","            print(f\" Error in step7_acoustic_preservation(): {e}\")\n","            print(\"You can try running with preserve_acoustics=False to isolate TTS issues.\")\n","            return\n","    else:\n","        print(\" Skipping acoustic preservation (preserve_acoustics=False).\")\n","        PIPELINE_DATA['preserved_audio'] = PIPELINE_DATA.get('synthesized_audio')\n","    # Step 8: metrics\n","    try:\n","        step8_performance_metrics()\n","    except Exception as e:\n","        print(f\" Error in step8_performance_metrics(): {e}\")\n","        return\n","    # Step 9: visualization\n","    try:\n","        step9_visualization()\n","    except Exception as e:\n","        print(f\" Error in step9_visualization(): {e}\")\n","    # Offer downloads if in Colab\n","    if IN_COLAB:\n","        print(\"\\n Preparing downloads (if files exist)...\")\n","        for f in ['synthesized_audio.wav', 'preserved_audio.wav', 'acoustic_comparison.png', 'radar_comparison.png', 'processing_time.png']:\n","            if os.path.exists(f):\n","                try:\n","                    print(f\" - {f}\")\n","                    colab_files.download(f)\n","                except Exception:\n","                    print(f\"   saved: {f}\")\n","    print(\"\\n FULL PIPELINE RUN COMPLETE\")\n","    print(\"   • Synthesized audio:\", PIPELINE_DATA.get('synthesized_audio'))\n","    print(\"   • Preserved audio:  \", PIPELINE_DATA.get('preserved_audio'))\n","    print(\"   • Metrics available: PIPELINE_DATA['metrics']\")\n","    print(\"=\"*70)\n","\n","# convenience wrapper for interactive upload + run\n","def upload_and_run(target_lang='es', force_tts=None, preserve_acoustics=True):\n","    p = upload_file_interactive()\n","    if not p:\n","        print(\"Upload failed or canceled.\")\n","        return\n","    ok = prepare_pipeline_with_file(p)\n","    if not ok:\n","        return\n","    run_full_pipeline(target_lang=target_lang, force_tts=force_tts, preserve_acoustics=preserve_acoustics)\n","\n","# Small interactive menu to help users run parts\n","def quick_menu():\n","    print(\"\\nPIPELINE QUICK MENU\")\n","    print(\"1. Upload file now and run full pipeline (default Spanish)\")\n","    print(\"2. Upload and run full pipeline (choose language)\")\n","    print(\"3. Run full pipeline using already uploaded file (default Spanish)\")\n","    print(\"4. Run full pipeline forcing Bark TTS (use if Coqui poor)\")\n","    print(\"5. Run full pipeline with preservation OFF (fast)\")\n","    print(\"6. Exit\")\n","    c = input(\"Choose an option [1-6]: \").strip()\n","    if c == '1':\n","        upload_and_run('es')\n","    elif c == '2':\n","        lang = input(\"Enter target language code (e.g. 'es','fr','de','hi'): \").strip()\n","        upload_and_run(lang)\n","    elif c == '3':\n","        run_full_pipeline('es')\n","    elif c == '4':\n","        upload_and_run('es', force_tts='bark')\n","    elif c == '5':\n","        upload_and_run('es', preserve_acoustics=False)\n","    else:\n","        print(\"Exiting menu.\")\n","\n","print(\"Helper runner loaded. Examples:\")\n","print(\" - upload_and_run(target_lang='es')           # interactive upload then run\")\n","print(\" - run_full_pipeline(target_lang='fr')         # run using PIPELINE_DATA['input_audio']\")\n","print(\" - quick_menu()                                # interactive menu\")\n","print(\" - upload_and_run(force_tts='bark')            # force Bark if Coqui is poor\")\n","print(\"\\nTip: If you changed the TTS or preservation methods, restart runtime and run cells 1-7 first to rebind functions/classes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUmheXekFIkm","executionInfo":{"status":"ok","timestamp":1762923992140,"user_tz":-330,"elapsed":44,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"813c31ff-7b25-473d-c933-852d8e2a147e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Helper runner loaded. Examples:\n"," - upload_and_run(target_lang='es')           # interactive upload then run\n"," - run_full_pipeline(target_lang='fr')         # run using PIPELINE_DATA['input_audio']\n"," - quick_menu()                                # interactive menu\n"," - upload_and_run(force_tts='bark')            # force Bark if Coqui is poor\n","\n","Tip: If you changed the TTS or preservation methods, restart runtime and run cells 1-7 first to rebind functions/classes.\n"]}]},{"cell_type":"code","source":["SUPPORTED_LANGUAGES['en']['model'] = 'Helsinki-NLP/opus-mt-hi-en'"],"metadata":{"id":"sN84i_SRkmrr","executionInfo":{"status":"ok","timestamp":1762923992149,"user_tz":-330,"elapsed":6,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Upload interactively and run everything (default target: English)\n","upload_and_run(target_lang='hi')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"nZZ9bYDDIv9B","executionInfo":{"status":"error","timestamp":1762927705435,"user_tz":-330,"elapsed":64,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}},"outputId":"e5d07a68-8e04-4192-eede-a4396ecda1f8"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'upload_and_run' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3399784048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Upload interactively and run everything (default target: English)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mupload_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'upload_and_run' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QNtBCy8JMotx","executionInfo":{"status":"ok","timestamp":1762924261614,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amit SINGH","userId":"07613553354720567103"}}},"execution_count":16,"outputs":[]}]}